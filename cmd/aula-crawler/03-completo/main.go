package main

import (
	"context"
	"encoding/csv"
	"fmt"
	"net/http"
	"net/url"
	"os"
	"sync"
	"time"

	"github.com/PuerkitoBio/goquery"
	"golang.org/x/time/rate"
)

/*
===========================================
EXEMPLO 3: CRAWLER COMPLETO (PROFISSIONAL)
===========================================

Este Ã© um crawler de nÃ­vel profissional com:
âœ… ConcorrÃªncia (Goroutines)
âœ… Rate Limiting (controle de requisiÃ§Ãµes por domÃ­nio)
âœ… ExportaÃ§Ã£o para CSV
âœ… Tratamento robusto de erros
âœ… EstatÃ­sticas detalhadas

ğŸ”¥ NOVO CONCEITO: RATE LIMITING

O que Ã© Rate Limiting?
- Limita a quantidade de requisiÃ§Ãµes por tempo
- Evita sobrecarregar servidores
- Previne bloqueios de IP
- Respeita polÃ­ticas de uso dos sites

Como funciona?
- Usamos golang.org/x/time/rate
- Cada domÃ­nio tem seu prÃ³prio limiter
- Exemplo: mÃ¡ximo 2 requisiÃ§Ãµes por segundo por domÃ­nio
*/

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ESTRUTURAS DE DADOS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

type Result struct {
	URL        string
	Title      string
	H1         string
	H2Count    int
	StatusCode int
	Duration   time.Duration
	Error      error
}

// DomainLimiter gerencia rate limiting por domÃ­nio
type DomainLimiter struct {
	limiters map[string]*rate.Limiter
	mu       sync.Mutex
}

// NewDomainLimiter cria um novo gerenciador de limiters
func NewDomainLimiter() *DomainLimiter {
	return &DomainLimiter{
		limiters: make(map[string]*rate.Limiter),
	}
}

// GetLimiter retorna ou cria um limiter para um domÃ­nio
func (dl *DomainLimiter) GetLimiter(domain string) *rate.Limiter {
	dl.mu.Lock()
	defer dl.mu.Unlock()

	// Se jÃ¡ existe limiter para este domÃ­nio, retorna
	if limiter, ok := dl.limiters[domain]; ok {
		return limiter
	}

	// Cria novo limiter: 2 requisiÃ§Ãµes por segundo, burst de 5
	// rate.Every(500*time.Millisecond) = 1 req a cada 500ms = 2 req/s
	limiter := rate.NewLimiter(rate.Every(500*time.Millisecond), 5)
	dl.limiters[domain] = limiter

	fmt.Printf("ğŸ”§ Criado rate limiter para domÃ­nio: %s (2 req/s)\n", domain)
	return limiter
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FUNÃ‡ÃƒO PRINCIPAL DE CRAWLING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

func fetchAndParse(urlStr string, dl *DomainLimiter) Result {
	startTime := time.Now()

	// 1. PARSE DA URL
	parsedURL, err := url.Parse(urlStr)
	if err != nil {
		return Result{URL: urlStr, Error: err}
	}

	// 2. RATE LIMITING
	// ObtÃ©m o limiter para este domÃ­nio
	limiter := dl.GetLimiter(parsedURL.Host)

	// Espera atÃ© ter permissÃ£o para fazer a requisiÃ§Ã£o
	fmt.Printf("â³ Aguardando rate limit para %s...\n", parsedURL.Host)
	err = limiter.Wait(context.Background())
	if err != nil {
		return Result{URL: urlStr, Error: err}
	}

	// 3. REQUISIÃ‡ÃƒO HTTP
	client := http.Client{
		Timeout: 10 * time.Second,
	}

	fmt.Printf("ğŸ” Buscando: %s\n", urlStr)
	resp, err := client.Get(urlStr)
	if err != nil {
		return Result{URL: urlStr, Error: err, Duration: time.Since(startTime)}
	}
	defer resp.Body.Close()

	// 4. PARSING DO HTML
	doc, err := goquery.NewDocumentFromReader(resp.Body)
	if err != nil {
		return Result{URL: urlStr, Error: err, StatusCode: resp.StatusCode, Duration: time.Since(startTime)}
	}

	// 5. EXTRAÃ‡ÃƒO DE DADOS
	title := doc.Find("title").First().Text()
	h1 := doc.Find("h1").First().Text()
	h2Count := doc.Find("h2").Length()

	duration := time.Since(startTime)

	return Result{
		URL:        urlStr,
		Title:      title,
		H1:         h1,
		H2Count:    h2Count,
		StatusCode: resp.StatusCode,
		Duration:   duration,
	}
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// WORKER COM RATE LIMITING
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

func worker(id int, jobs <-chan string, results chan<- Result, dl *DomainLimiter, wg *sync.WaitGroup) {
	defer wg.Done()

	for urlStr := range jobs {
		fmt.Printf("ğŸ¤– Worker %d processando: %s\n", id, urlStr)
		result := fetchAndParse(urlStr, dl)
		results <- result
	}

	fmt.Printf("ğŸ‘‹ Worker %d finalizou!\n", id)
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// EXPORTAÃ‡ÃƒO PARA CSV
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

func exportToCSV(results []Result, filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	defer writer.Flush()

	// CabeÃ§alho
	header := []string{"URL", "Title", "H1", "H2_Count", "Status_Code", "Duration_ms", "Error"}
	if err := writer.Write(header); err != nil {
		return err
	}

	// Dados
	for _, r := range results {
		errorStr := ""
		if r.Error != nil {
			errorStr = r.Error.Error()
		}

		row := []string{
			r.URL,
			r.Title,
			r.H1,
			fmt.Sprintf("%d", r.H2Count),
			fmt.Sprintf("%d", r.StatusCode),
			fmt.Sprintf("%.0f", r.Duration.Seconds()*1000),
			errorStr,
		}

		if err := writer.Write(row); err != nil {
			return err
		}
	}

	return nil
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FUNÃ‡ÃƒO MAIN
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

func main() {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘   WEB CRAWLER PROFISSIONAL (COM RATE LIMITING)    â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()

	// URLs de diferentes domÃ­nios
	urls := []string{
		// Mesmo domÃ­nio - serÃ¡ limitado
		"https://golang.org",
		"https://golang.org/doc",
		"https://golang.org/pkg",

		// Mesmo domÃ­nio - serÃ¡ limitado
		"https://go.dev",
		"https://go.dev/learn",
		"https://go.dev/solutions",

		// DomÃ­nios diferentes
		"https://github.com/golang",
		"https://stackoverflow.com/questions/tagged/go",
		"https://pkg.go.dev",
		"https://play.golang.org",
	}

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// CONFIGURAÃ‡ÃƒO
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	workerCount := 5
	domainLimiter := NewDomainLimiter()

	jobs := make(chan string, len(urls))
	results := make(chan Result, len(urls))
	var wg sync.WaitGroup

	fmt.Printf("ğŸš€ Iniciando %d workers...\n", workerCount)
	fmt.Printf("ğŸ“Š URLs para processar: %d\n", len(urls))
	fmt.Printf("âš¡ Rate limit: 2 requisiÃ§Ãµes/segundo por domÃ­nio\n\n")

	startTime := time.Now()

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// INICIAR WORKERS
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	for i := 1; i <= workerCount; i++ {
		wg.Add(1)
		go worker(i, jobs, results, domainLimiter, &wg)
	}

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// ENVIAR JOBS
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	for _, url := range urls {
		jobs <- url
	}
	close(jobs)

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// COLETAR RESULTADOS
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	go func() {
		wg.Wait()
		close(results)
	}()

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// PROCESSAR RESULTADOS
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	var allResults []Result
	successCount := 0
	errorCount := 0
	totalDuration := time.Duration(0)

	fmt.Println("\nğŸ“Š RESULTADOS:")
	fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

	for result := range results {
		allResults = append(allResults, result)

		if result.Error != nil {
			fmt.Printf("âŒ %s\n   Erro: %v\n   DuraÃ§Ã£o: %v\n\n",
				result.URL, result.Error, result.Duration)
			errorCount++
		} else {
			fmt.Printf("âœ… %s\n", result.URL)
			fmt.Printf("   ğŸ“„ TÃ­tulo: %s\n", result.Title)
			fmt.Printf("   ğŸ“Œ H1: %s\n", result.H1)
			fmt.Printf("   ğŸ”¢ H2s: %d\n", result.H2Count)
			fmt.Printf("   ğŸŒ Status: %d\n", result.StatusCode)
			fmt.Printf("   â±ï¸  DuraÃ§Ã£o: %v\n\n", result.Duration)
			successCount++
			totalDuration += result.Duration
		}
	}

	elapsed := time.Since(startTime)

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// EXPORTAR PARA CSV
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	csvFilename := "crawler_results.csv"
	if err := exportToCSV(allResults, csvFilename); err != nil {
		fmt.Printf("âŒ Erro ao exportar CSV: %v\n", err)
	} else {
		fmt.Printf("ğŸ’¾ Resultados exportados para: %s\n\n", csvFilename)
	}

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// ESTATÃSTICAS FINAIS
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	avgDuration := time.Duration(0)
	if successCount > 0 {
		avgDuration = totalDuration / time.Duration(successCount)
	}

	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Printf("â•‘   TEMPO TOTAL: %-35s  â•‘\n", elapsed)
	fmt.Printf("â•‘   URLs PROCESSADAS: %-30d  â•‘\n", len(urls))
	fmt.Printf("â•‘   Workers: %-39d  â•‘\n", workerCount)
	fmt.Printf("â•‘   Sucessos: %-38d  â•‘\n", successCount)
	fmt.Printf("â•‘   Erros: %-41d  â•‘\n", errorCount)
	fmt.Printf("â•‘   DuraÃ§Ã£o mÃ©dia por URL: %-25s  â•‘\n", avgDuration)
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()

	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	// EXPLICAÃ‡ÃƒO DO RATE LIMITING
	// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
	fmt.Println("ğŸ’¡ COMO O RATE LIMITING FUNCIONOU?")
	fmt.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()
	fmt.Println("ğŸŒ DOMÃNIOS PROCESSADOS:")
	fmt.Println("   golang.org: 3 URLs (limitadas a 2 req/s)")
	fmt.Println("   go.dev: 3 URLs (limitadas a 2 req/s)")
	fmt.Println("   Outros: 4 URLs (cada um com seu limiter)")
	fmt.Println()
	fmt.Println("â±ï¸  SEM RATE LIMIT:")
	fmt.Println("   - Risco de IP bloqueado")
	fmt.Println("   - PossÃ­vel sobrecarga do servidor")
	fmt.Println("   - Comportamento antiÃ©tico")
	fmt.Println()
	fmt.Println("âœ… COM RATE LIMIT:")
	fmt.Println("   - Respeita o servidor")
	fmt.Println("   - Evita bloqueios")
	fmt.Println("   - Comportamento profissional")
	fmt.Println()
	fmt.Println("ğŸ“ˆ BENEFÃCIOS DA CONCORRÃŠNCIA + RATE LIMITING:")
	fmt.Println("   - Processa mÃºltiplos domÃ­nios em paralelo")
	fmt.Println("   - Respeita limites de cada domÃ­nio")
	fmt.Println("   - MÃ¡xima velocidade com responsabilidade")
	fmt.Println()
	fmt.Println("ğŸ¯ CASOS DE USO REAIS:")
	fmt.Println("   âœ“ Web scraping de produtos")
	fmt.Println("   âœ“ Monitoramento de SEO")
	fmt.Println("   âœ“ AgregaÃ§Ã£o de notÃ­cias")
	fmt.Println("   âœ“ AnÃ¡lise de competidores")
	fmt.Println("   âœ“ IndexaÃ§Ã£o de conteÃºdo")
}
