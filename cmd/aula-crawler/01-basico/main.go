package main

import (
	"fmt"
	"net/http"
	"time"

	"github.com/PuerkitoBio/goquery"
)

/*
===========================================
EXEMPLO 1: CRAWLER BÃSICO (SÃNCRONO)
===========================================

Este Ã© o exemplo mais simples de um web crawler.
Ele busca pÃ¡ginas uma por vez, de forma sequencial.

CONCEITOS IMPORTANTES:
- HTTP Client: Faz requisiÃ§Ãµes HTTP
- Timeout: Define tempo mÃ¡ximo de espera
- HTML Parsing: Extrai dados do HTML
- Error Handling: Tratamento de erros
*/

// Result representa o resultado de uma requisiÃ§Ã£o
type Result struct {
	URL   string // URL que foi acessada
	Title string // TÃ­tulo da pÃ¡gina (<title>)
	H1    string // Primeiro H1 encontrado
	Error error  // Erro (se houver)
}

// fetchAndParse busca uma URL e extrai informaÃ§Ãµes
func fetchAndParse(url string) Result {
	fmt.Printf("ğŸ” Buscando: %s\n", url)

	// 1. CRIANDO O CLIENT HTTP
	// Timeout: se nÃ£o responder em 10 segundos, cancela
	client := http.Client{
		Timeout: 10 * time.Second,
	}

	// 2. FAZENDO A REQUISIÃ‡ÃƒO GET
	resp, err := client.Get(url)
	if err != nil {
		return Result{URL: url, Error: err}
	}
	defer resp.Body.Close() // IMPORTANTE: sempre fechar o body

	// 3. PARSEANDO O HTML
	// goquery Ã© como jQuery para Go
	doc, err := goquery.NewDocumentFromReader(resp.Body)
	if err != nil {
		return Result{URL: url, Error: err}
	}

	// 4. EXTRAINDO DADOS
	// Find() busca elementos CSS, como em jQuery
	title := doc.Find("title").First().Text()
	h1 := doc.Find("h1").First().Text()

	return Result{
		URL:   url,
		Title: title,
		H1:    h1,
	}
}

func main() {
	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Println("â•‘   WEB CRAWLER BÃSICO (SÃNCRONO)          â•‘")
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()

	// Lista de URLs para crawlear
	urls := []string{
		"https://golang.org",
		"https://go.dev",
		"https://github.com",
		"https://stackoverflow.com",
	}

	// Marca o tempo de inÃ­cio
	startTime := time.Now()

	// PROCESSAMENTO SÃNCRONO
	// Uma URL por vez, esperando cada uma terminar
	for _, url := range urls {
		result := fetchAndParse(url)

		if result.Error != nil {
			fmt.Printf("âŒ Erro ao buscar %s: %v\n\n", result.URL, result.Error)
		} else {
			fmt.Printf("âœ… URL: %s\n", result.URL)
			fmt.Printf("   ğŸ“„ TÃ­tulo: %s\n", result.Title)
			fmt.Printf("   ğŸ“Œ H1: %s\n\n", result.H1)
		}
	}

	// Calcula o tempo total
	elapsed := time.Since(startTime)

	fmt.Println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
	fmt.Printf("â•‘   TEMPO TOTAL: %-24s  â•‘\n", elapsed)
	fmt.Printf("â•‘   URLs PROCESSADAS: %-19d  â•‘\n", len(urls))
	fmt.Println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Println()
	fmt.Println("âš ï¸  PROBLEMA: Este crawler Ã© LENTO!")
	fmt.Println("    Processa uma URL por vez, esperando cada uma terminar.")
	fmt.Println("    Para 4 URLs com 2s cada = 8 segundos totais")
	fmt.Println()
	fmt.Println("ğŸ’¡ SOLUÃ‡ÃƒO: Use concorrÃªncia com Goroutines!")
	fmt.Println("    Execute: go run cmd/aula-crawler/02-concorrente/main.go")
}
