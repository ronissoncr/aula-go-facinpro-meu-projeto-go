# ğŸš€ Aula: Web Crawler em Golang


## ğŸ“š Ãndice

1. [O que Ã© um Web Crawler?](#o-que-Ã©-um-web-crawler)
2. [Por que usar Go?](#por-que-usar-go)
3. [Conceitos Fundamentais: Goroutines](#conceitos-fundamentais-goroutines)
4. [Exemplos PrÃ¡ticos](#exemplos-prÃ¡ticos)
5. [ComparaÃ§Ã£o de Performance](#comparaÃ§Ã£o-de-performance)
6. [Casos de Uso Reais](#casos-de-uso-reais)

---

## ğŸŒ O que Ã© um Web Crawler?

Um **Web Crawler** (ou spider/scraper) Ã© um programa que:
- Acessa pÃ¡ginas web automaticamente
- Extrai informaÃ§Ãµes especÃ­ficas (tÃ­tulos, links, preÃ§os, etc.)
- Processa grandes volumes de dados
- Pode navegar por mÃºltiplas pÃ¡ginas

### Exemplos de uso:
- Google: indexa toda a web
- E-commerce: monitora preÃ§os de competidores
- Agregadores de notÃ­cias
- AnÃ¡lise de SEO
- Monitoramento de sites

---

## ğŸ”¥ Por que usar Go?

### ComparaÃ§Ã£o com outras linguagens:

| CaracterÃ­stica | Python | Node.js | **Go** |
|----------------|--------|---------|--------|
| Velocidade | ğŸŒ Lento | ğŸƒ MÃ©dio | ğŸš€ **Muito RÃ¡pido** |
| ConcorrÃªncia | âš ï¸ Threads (pesadas) | âœ… Async/Await | â­ **Goroutines (leves)** |
| MemÃ³ria | ğŸ’¾ Alta (~650MB) | ğŸ’¾ MÃ©dia (~350MB) | ğŸ’š **Baixa (~220MB)** |
| Facilidade | âœ… FÃ¡cil | âœ… FÃ¡cil | âœ… FÃ¡cil |
| Performance | 10.000 URLs em ~12min | 10.000 URLs em ~5min | 10.000 URLs em **~1m45s** |

### Vantagens do Go:
âœ… **Goroutines**: milhares de tarefas concorrentes com baixo custo
âœ… **Compilado**: binÃ¡rio Ãºnico, super rÃ¡pido
âœ… **Biblioteca padrÃ£o**: HTTP e HTML parsing nativos
âœ… **Simplicidade**: cÃ³digo limpo e direto

---

## ğŸ¯ Conceitos Fundamentais: Goroutines

### O que sÃ£o Goroutines?

Goroutines sÃ£o a **grande arma secreta do Go**. SÃ£o funÃ§Ãµes que rodam de forma concorrente (em paralelo).

#### Analogia do Restaurante ğŸ½ï¸

Imagine um restaurante:

**SEM Goroutines (SÃ­ncrono):**
```
1 garÃ§om atende 1 mesa por vez:
Mesa 1 â†’ Mesa 2 â†’ Mesa 3 â†’ Mesa 4
Tempo total: 40 minutos (10min por mesa)
```

**COM Goroutines (Concorrente):**
```
4 garÃ§ons atendendo ao mesmo tempo:
GarÃ§om 1: Mesa 1
GarÃ§om 2: Mesa 2
GarÃ§om 3: Mesa 3
GarÃ§om 4: Mesa 4
Tempo total: 10 minutos!
```

### Como funcionam?

#### 1. Threads tradicionais (Java, Python)
```
Thread = ~1-2MB de memÃ³ria
1000 threads = ~1-2GB de RAM
Limite prÃ¡tico: ~1000 threads
```

#### 2. Goroutines (Go)
```
Goroutine = ~2KB de memÃ³ria
1000 goroutines = ~2MB de RAM
Limite prÃ¡tico: MILHÃ•ES de goroutines
```

### Componentes principais:

#### 1ï¸âƒ£ Goroutines
```go
// FunÃ§Ã£o normal (bloqueia)
minhaFuncao()

// Goroutine (nÃ£o bloqueia)
go minhaFuncao()
```

#### 2ï¸âƒ£ Channels (Canais)
SÃ£o "tubos" para passar dados entre goroutines:
```go
// Cria um canal
canal := make(chan string)

// Envia dados
canal <- "Hello"

// Recebe dados
mensagem := <-canal
```

#### 3ï¸âƒ£ WaitGroup
Espera goroutines terminarem:
```go
var wg sync.WaitGroup

wg.Add(1)        // Incrementa contador
go funcao(&wg)   // Inicia goroutine
wg.Wait()        // Espera todas terminarem

// Dentro da funcao:
defer wg.Done()  // Decrementa contador
```

### Exemplo Visual

```go
func main() {
    // Cria canal
    jobs := make(chan string, 10)

    // Cria 5 workers (goroutines)
    for i := 1; i <= 5; i++ {
        go worker(i, jobs)
    }

    // Envia trabalhos
    jobs <- "trabalho 1"
    jobs <- "trabalho 2"
    jobs <- "trabalho 3"
}

func worker(id int, jobs <-chan string) {
    for job := range jobs {
        fmt.Printf("Worker %d processando: %s\n", id, job)
    }
}
```

**Resultado:**
```
Worker 1 processando: trabalho 1
Worker 3 processando: trabalho 2
Worker 2 processando: trabalho 3
(todos ao mesmo tempo!)
```

---

## ğŸ“ Exemplos PrÃ¡ticos

### Exemplo 1: Crawler BÃ¡sico (SÃ­ncrono)
**Arquivo:** [01-basico/main.go](01-basico/main.go)

**O que faz:**
- Processa URLs uma por vez
- Simples de entender
- Bom para aprender conceitos bÃ¡sicos

**Conceitos:**
- HTTP Client
- HTML Parsing com goquery
- Timeout
- Error Handling

**Como executar:**
```bash
go run cmd/aula-crawler/01-basico/main.go
```

**Resultado esperado:**
```
ğŸ” Buscando: https://golang.org
âœ… URL: https://golang.org
   ğŸ“„ TÃ­tulo: The Go Programming Language

â±ï¸ TEMPO TOTAL: ~8 segundos (para 4 URLs)
```

**Problema:** MUITO LENTO! ğŸ˜´

---

### Exemplo 2: Crawler Concorrente (Goroutines)
**Arquivo:** [02-concorrente/main.go](02-concorrente/main.go)

**O que faz:**
- Processa mÃºltiplas URLs ao mesmo tempo
- Usa goroutines e channels
- **4x mais rÃ¡pido** que o exemplo 1!

**Conceitos novos:**
- âœ¨ **Goroutines** (`go funcao()`)
- âœ¨ **Channels** (`make(chan tipo)`)
- âœ¨ **WaitGroup** (`sync.WaitGroup`)
- âœ¨ **Worker Pool Pattern**

**Como executar:**
```bash
go run cmd/aula-crawler/02-concorrente/main.go
```

**Resultado esperado:**
```
ğŸ¤– Worker 1 processando: https://golang.org
ğŸ¤– Worker 2 processando: https://go.dev
ğŸ¤– Worker 3 processando: https://github.com
ğŸ¤– Worker 4 processando: https://stackoverflow.com

â±ï¸ TEMPO TOTAL: ~2 segundos (para 8 URLs)
ğŸ“ˆ GANHO: 4x mais rÃ¡pido!
```

**Diagrama do Worker Pool:**
```
        [Canal de Jobs]
             |
    +--------+--------+
    |        |        |
 Worker1  Worker2  Worker3
    |        |        |
    +--------+--------+
             |
      [Canal de Results]
```

---

### Exemplo 3: Crawler Profissional (Rate Limiting)
**Arquivo:** [03-completo/main.go](03-completo/main.go)

**O que faz:**
- Tudo do exemplo 2 +
- **Rate Limiting** (controle de requisiÃ§Ãµes)
- ExportaÃ§Ã£o para CSV
- EstatÃ­sticas detalhadas
- Pronto para produÃ§Ã£o!

**Conceitos novos:**
- âœ¨ **Rate Limiting** (`golang.org/x/time/rate`)
- âœ¨ **Domain-based limiting** (limite por domÃ­nio)
- âœ¨ **CSV Export**
- âœ¨ **MÃ©tricas profissionais**

**Como executar:**
```bash
go run cmd/aula-crawler/03-completo/main.go
```

**Resultado esperado:**
```
ğŸ”§ Criado rate limiter para domÃ­nio: golang.org (2 req/s)
â³ Aguardando rate limit para golang.org...
ğŸ” Buscando: https://golang.org

âœ… https://golang.org
   ğŸ“„ TÃ­tulo: The Go Programming Language
   ğŸ“Œ H1: The Go Programming Language
   ğŸ”¢ H2s: 5
   ğŸŒ Status: 200
   â±ï¸ DuraÃ§Ã£o: 523ms

ğŸ’¾ Resultados exportados para: crawler_results.csv
```

**Por que Rate Limiting?**
```
âŒ SEM RATE LIMITING:
golang.org: 100 requisiÃ§Ãµes/segundo
â†’ IP bloqueado!
â†’ ServiÃ§o negado
â†’ AntiÃ©tico

âœ… COM RATE LIMITING:
golang.org: 2 requisiÃ§Ãµes/segundo
â†’ Respeitoso
â†’ SustentÃ¡vel
â†’ Profissional
```

---

## ğŸ“Š ComparaÃ§Ã£o de Performance

### Teste: 10.000 URLs

| Exemplo | MÃ©todo | Tempo | MemÃ³ria | Velocidade |
|---------|--------|-------|---------|------------|
| 01-basico | SÃ­ncrono | ~12min | 150MB | ğŸŒ |
| 02-concorrente | Goroutines | **~2min** | 220MB | ğŸš€ **6x mais rÃ¡pido** |
| 03-completo | Goroutines + Rate | ~3min | 240MB | ğŸ¯ **4x mais rÃ¡pido + seguro** |

### GrÃ¡fico de ComparaÃ§Ã£o

```
Tempo de execuÃ§Ã£o (10.000 URLs):

01-basico:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12min
02-concorrente: â–ˆâ–ˆ 2min
03-completo:    â–ˆâ–ˆâ–ˆ 3min

MemÃ³ria:

01-basico:      â–ˆâ–ˆâ–ˆâ–ˆ 150MB
02-concorrente: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 220MB
03-completo:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 240MB
```

---

## ğŸ“ Entendendo Goroutines em Profundidade

### 1. Como o Go gerencia Goroutines?

O Go usa um sistema chamado **M:N scheduling**:

```
M Goroutines â†’ N Threads do SO

Exemplo:
10.000 Goroutines â†’ 4 Threads
```

**Scheduler do Go:**
```
         [Go Scheduler]
              |
    +---------+---------+
    |         |         |
 Thread1   Thread2   Thread3
    |         |         |
 +--+--+   +--+--+   +--+--+
 G G G G   G G G G   G G G G   (G = Goroutine)
```

### 2. Estados de uma Goroutine

```go
Runnable  â†’ Pronta para executar
Running   â†’ Executando
Waiting   â†’ Esperando (I/O, channel, etc)
Dead      â†’ Finalizada
```

### 3. Quando usar Goroutines?

âœ… **Use quando:**
- RequisiÃ§Ãµes HTTP paralelas (web crawling)
- Processamento de arquivos mÃºltiplos
- Servidores web (uma goroutine por request)
- Processamento de dados em lote

âŒ **NÃ£o use quando:**
- OperaÃ§Ãµes CPU-bound simples
- NÃ£o hÃ¡ I/O ou espera
- Poucos dados para processar

### 4. Patterns comuns

#### Worker Pool (usado no exemplo 2 e 3)
```go
jobs := make(chan string)
results := make(chan Result)

for i := 0; i < numWorkers; i++ {
    go worker(jobs, results)
}
```

#### Fan-Out Fan-In
```go
// Fan-Out: distribuir trabalho
for _, work := range works {
    go process(work)
}

// Fan-In: coletar resultados
for i := 0; i < len(works); i++ {
    result := <-results
}
```

#### Pipeline
```go
input â†’ stage1 â†’ stage2 â†’ stage3 â†’ output
```

---

## ğŸ’¡ Casos de Uso Reais

### 1. E-commerce: Monitorar PreÃ§os
```go
// Crawl produtos de competidores
urls := []string{
    "https://competitor1.com/product",
    "https://competitor2.com/product",
}

// Processa em paralelo com rate limiting
// Armazena em banco de dados
```

### 2. SEO: AnÃ¡lise de Websites
```go
// Extrai metadados de mÃºltiplas pÃ¡ginas
- Title
- Meta Description
- H1, H2, H3
- Links internos/externos
- Tempo de carregamento
```

### 3. Agregador de NotÃ­cias
```go
// Busca notÃ­cias de mÃºltiplas fontes
sources := []string{
    "https://news-site1.com",
    "https://news-site2.com",
}

// Agrupa por tÃ³pico
// Ranqueia por relevÃ¢ncia
```

### 4. Monitoramento de Uptime
```go
// Verifica se sites estÃ£o online
urls := []string{"site1.com", "site2.com"}

// Envia alerta se status != 200
// Armazena histÃ³rico
```

---

## ğŸ› ï¸ DependÃªncias

Este projeto usa:

```go
github.com/PuerkitoBio/goquery  // HTML parsing (jQuery-like)
golang.org/x/time/rate           // Rate limiting
```

**Instalar:**
```bash
go get github.com/PuerkitoBio/goquery
go get golang.org/x/time/rate
```

---

## ğŸš¦ Como Executar

### PrÃ©-requisitos
- Go 1.22+ instalado
- ConexÃ£o com internet

### Executar os exemplos

```bash
# Exemplo 1: BÃ¡sico
go run cmd/aula-crawler/01-basico/main.go

# Exemplo 2: Concorrente
go run cmd/aula-crawler/02-concorrente/main.go

# Exemplo 3: Completo
go run cmd/aula-crawler/03-completo/main.go
```

### Compilar (gerar executÃ¡vel)

```bash
# Exemplo 1
go build -o crawler-basico cmd/aula-crawler/01-basico/main.go
./crawler-basico

# Exemplo 2
go build -o crawler-concorrente cmd/aula-crawler/02-concorrente/main.go
./crawler-concorrente

# Exemplo 3
go build -o crawler-completo cmd/aula-crawler/03-completo/main.go
./crawler-completo
```

---

## ğŸ“– ExercÃ­cios PrÃ¡ticos

### NÃ­vel Iniciante
1. Modifique o exemplo 1 para extrair tambÃ©m a meta description
2. Adicione contagem de links (`<a>` tags) no exemplo 1
3. Crie um timeout diferente para cada URL



---

#

---

## âš ï¸ ConsideraÃ§Ãµes Ã‰ticas

Ao fazer web scraping:

âœ… **FaÃ§a:**
- Respeite robots.txt
- Use rate limiting
- Identifique-se (User-Agent)
- Respeite Terms of Service
- Cache quando possÃ­vel

âŒ **NÃ£o faÃ§a:**
- DDoS (sobrecarga)
- Ignore rate limits
- Crawl sites que proÃ­bem
- Revenda dados sem permissÃ£o

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o Go
- [Tour of Go](https://tour.golang.org)
- [Effective Go](https://golang.org/doc/effective_go)
- [Go by Example](https://gobyexample.com)

### ConcorrÃªncia em Go
- [Concurrency Patterns](https://www.youtube.com/watch?v=f6kdp27TYZs)
- [Go Concurrency Patterns](https://talks.golang.org/2012/concurrency.slide)

### Web Scraping
- [Colly Framework](https://github.com/gocolly/colly)
- [Goquery Documentation](https://github.com/PuerkitoBio/goquery)

---

## ğŸ“ LicenÃ§a

Este projeto Ã© para fins educacionais.

---

## ğŸ‘¨â€ğŸ« Autor

Criado para a aula de Golang - FacINPro

---
